{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import random\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, silhouette_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import keras_tuner\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification, XLNetConfig, TFXLNetModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import XLNetForSequenceClassification, pipeline, TextClassificationPipeline\n",
    "from transformers import AdamW, get_scheduler\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "import sys\n",
    "sys.path.append(\"/home/alexxgo21/workspace/thesis/modules\")\n",
    "from preprocess_raw_html import preprocess_raw_html\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_parquet(\"../dataset/philosophy-qna-with-gpt35answer_v1.parquet\")\n",
    "# get the columns of interest\n",
    "df = df[[\"view_count\",\"answer_count\",\"is_accepted\",\"answer_score\",\"answer_creation_date\",\"answers\",\"question_score\",\"question_creation_date\",\"link\",\"question\",\"title\",\"gpt35_0125_ans\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get two random index if the answer array contains more than 2 answers\n",
    "def select_random_index(arr,max_ans=2):\n",
    "    if len(arr) > max_ans:\n",
    "        return random.sample(range(len(arr)), max_ans)\n",
    "    return None\n",
    "\n",
    "# function to update the affected columns in the dataframe based on the selected random index\n",
    "def update_affected_columns(row):\n",
    "    if row[\"temp_index\"] is not None:\n",
    "        row[\"answers\"] = np.array(row[\"answers\"])[row[\"temp_index\"]]\n",
    "        row[\"is_accepted\"] = np.array(row[\"is_accepted\"])[row[\"temp_index\"]]\n",
    "        row[\"answer_score\"] = np.array(row[\"answer_score\"])[row[\"temp_index\"]]\n",
    "        row[\"answer_creation_date\"] = np.array(row[\"answer_creation_date\"])[row[\"temp_index\"]]\n",
    "    return row\n",
    "\n",
    "# function to get the gpt answer from the json response\n",
    "def get_gpt_answer_from_json_response(text):\n",
    "    return re.sub(r\"\\n\", \" \", text[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove special characters and numbers\n",
    "    text = re.sub(\"[^0-9a-zA-Z]+\", \" \", text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the number of answers per question to 2\n",
    "df[\"temp_index\"] = df[\"answers\"].apply(lambda x: select_random_index(x))\n",
    "# update affected columns\n",
    "df = df.apply(lambda x: update_affected_columns(x), axis=1)\n",
    "# drop the temp_index column\n",
    "df = df.drop(columns=[\"temp_index\"])\n",
    "\n",
    "df_ans_exploded = df[[\"answers\",\"question\",\"title\",\"gpt35_0125_ans\"]]\n",
    "# explode the answers column\n",
    "df_ans_exploded = df_ans_exploded.explode(\"answers\").reset_index(drop=True)\n",
    "\n",
    "# preprocess the raw html\n",
    "df_ans_exploded[\"answers\"] = df_ans_exploded[\"answers\"].apply(preprocess_raw_html)\n",
    "# get the gpt answer from the json response\n",
    "df_ans_exploded[\"gpt_response\"] = df_ans_exploded[\"gpt35_0125_ans\"].apply(get_gpt_answer_from_json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_response = list(df_ans_exploded[\"answers\"].drop_duplicates())\n",
    "human_response = [preprocess_text(text) for text in human_response]\n",
    "gpt_response = list(df_ans_exploded[\"gpt_response\"].drop_duplicates())\n",
    "gpt_response = [preprocess_text(text) for text in gpt_response]\n",
    "\n",
    "# 1 - human-generated, 0 - gpt-generated\n",
    "Y = [1 for _ in range(len(human_response))] + [0 for _ in range(len(gpt_response))] \n",
    "X = human_response + gpt_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# train, test, and val split with StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3)\n",
    "for train_index, test_index in sss.split(X,Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5)\n",
    "for val_index, test_index in sss.split(X_test,Y_test):\n",
    "    X_val, X_test = X_test[val_index], X_test[test_index]\n",
    "    Y_val, Y_test = Y_test[val_index], Y_test[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'important concept statistical experiments issue confounding variable researchers making common mistake assuming correlation implies causation whereas many examples explicitly demonstrate true host methods meant avoid kind mistake like control pairing subjects etc know particularly formal formulation error called false cause fallacy'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "Name: /physical_device:GPU:0 Type: GPU\n",
      "Physical devices cannot be modified after being initialized\n",
      "Matrix multiplication result: tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "Runs on GPU: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. Check if TensorFlow sees any GPUs\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# 2. List GPU devices with details\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"Type:\", gpu.device_type)\n",
    "    \n",
    "# 3. Get GPU device details\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# 4. Test GPU with simple operation\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(\"Matrix multiplication result:\", c)\n",
    "    print(\"Runs on GPU:\", c.device.endswith('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(batch:list, labels:list, batch_size:int, tokenizer:XLNetTokenizer, max_len:int=128):\n",
    "    # tokenize the batch\n",
    "    tokenized_batch = tokenizer(\n",
    "        batch, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128, \n",
    "        return_tensors=\"tf\",\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    # one-hot encode the labels\n",
    "    labels = tf.one_hot(labels, depth=2)\n",
    "    # create a dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                'input_ids': tokenized_batch['input_ids'],\n",
    "                'attention_mask': tokenized_batch['attention_mask'],\n",
    "            },\n",
    "            labels\n",
    "        ))\n",
    "    \n",
    "    return dataset.shuffle(100).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "train_data = preprocess_dataset(list(X_train), list(Y_train), batch_size, tokenizer)\n",
    "val_data = preprocess_dataset(list(X_val), list(Y_val), batch_size, tokenizer)\n",
    "test_data = preprocess_dataset(list(X_test), list(Y_test), batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet = TFXLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "word_inputs = tf.keras.Input(shape=(128,), name='word_inputs', dtype='int32')\n",
    "xlnet_encodings = xlnet(word_inputs)[0]\n",
    "\n",
    "# Collect last step from last hidden state (CLS)\n",
    "doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n",
    "\n",
    "doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(doc_encoding)\n",
    "\n",
    "model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 21:23:18.044684: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 98304000 exceeds 10% of free system memory.\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetModel: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class XLNetClassifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes=2, dropout_rate=0.1):\n",
    "        super(XLNetClassifier, self).__init__()\n",
    "        \n",
    "        # self.config = XLNetConfig.from_pretrained(\"xlnet-base-cased\", num_labels=num_classes)\n",
    "        xlnet = TFXLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
    "        # for layer in self.xlnet.layers:\n",
    "        #     layer.trainable = False\n",
    "        word_inputs = tf.keras.Input(shape=(128,), name='word_inputs', dtype='int32')\n",
    "        xlnet_encodings = xlnet(word_inputs)[0]\n",
    "\n",
    "        # Collect last step from last hidden state (CLS)\n",
    "        doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n",
    "    \n",
    "        doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n",
    "     \n",
    "        outputs = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(doc_encoding)\n",
    "\n",
    "        model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n",
    "    \n",
    "\n",
    "    def call(self, inputs):\n",
    "        xlnet_output = self.xlnet(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "        )[0]\n",
    "\n",
    "        cls_output = xlnet_output[:, -1, :]\n",
    "        \n",
    "        return self.classifier(cls_output)\n",
    "\n",
    "model = XLNetClassifier(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 21:23:38.590868: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 98304000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4d98ec1d90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SubclassModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', save_best_only=True):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.best = float('inf') if mode == 'min' else float('-inf')\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        \n",
    "    def _is_improvement(self, current):\n",
    "        if self.mode == 'min':\n",
    "            return current < self.best\n",
    "        return current > self.best\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "            \n",
    "        if self.save_best_only:\n",
    "            if self._is_improvement(current):\n",
    "                self.best = current\n",
    "                self.model.save_weights(self.filepath)\n",
    "                print(f'\\nEpoch {epoch+1}: {self.monitor} improved to {current:.4f}, saving model to {self.filepath}')\n",
    "        else:\n",
    "            filepath = f\"{self.filepath}_epoch_{epoch+1}\"\n",
    "            self.model.save_weights(filepath)\n",
    "            print(f'\\nEpoch {epoch+1}: saving model to {filepath}')\n",
    "\n",
    "# class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "#     def __init__(self, initial_lr, warmup_steps, decay_steps):\n",
    "#         super().__init__()\n",
    "#         self.initial_lr = initial_lr\n",
    "#         self.warmup_steps = warmup_steps\n",
    "#         self.decay_steps = decay_steps\n",
    "        \n",
    "#     def __call__(self, step):\n",
    "#         # Warmup phase\n",
    "#         warmup_pct = tf.math.minimum(1.0, step / self.warmup_steps)\n",
    "#         warmup_lr = self.initial_lr * warmup_pct\n",
    "        \n",
    "#         # Cosine decay phase\n",
    "#         progress = (step - self.warmup_steps) / (self.decay_steps - self.warmup_steps)\n",
    "#         cosine_decay = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        \n",
    "#         # Combine warmup and decay\n",
    "#         return tf.where(step < self.warmup_steps, warmup_lr, \n",
    "#                        self.initial_lr * cosine_decay)\n",
    "\n",
    "epochs = 100\n",
    "# total_steps = (len(train_data) // batch_size) * epochs\n",
    "# warmup_steps = total_steps // 10\n",
    "\n",
    "# lr_schedule = WarmupCosineDecay(\n",
    "#     initial_lr=2e-5,\n",
    "#     warmup_steps=warmup_steps,\n",
    "#     decay_steps=total_steps\n",
    "# )\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "checkpoint_path = \"./checkpoints/best_model_weights\"\n",
    "# checkpoint_callback = SubclassModelCheckpoint(\n",
    "#     filepath=checkpoint_path,\n",
    "#     monitor='val_accuracy',\n",
    "#     mode='max',\n",
    "#     save_best_only=True\n",
    "# )\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping with patience\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        mode='min'\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint\n",
    "    SubclassModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='./logs',\n",
    "        update_freq='epoch',\n",
    "        profile_batch=0\n",
    "    ),\n",
    "    \n",
    "    # Reduce LR on plateau\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# Load best weights after training\n",
    "# model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 13s 153ms/step - loss: 0.2421 - accuracy: 0.9158 - precision_1: 0.9158 - recall_1: 0.9158 - auc: 0.9710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24209408462047577,\n",
       " 0.9158316850662231,\n",
       " 0.9158316850662231,\n",
       " 0.9158316850662231,\n",
       " 0.9709819555282593]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

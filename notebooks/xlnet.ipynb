{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFXLNetModel, XLMTokenizer\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Choose model and tokenizer (English XLNet for this example)\n",
    "model_name = \"xlnet-base-cased\"\n",
    "tokenizer = XLMTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load and preprocess data\n",
    "train_texts, train_labels = ...  # Load your data (clean and tokenize)\n",
    "test_texts, test_labels = ...  # Load your test data (clean and tokenize)\n",
    "\n",
    "# Convert text to token IDs\n",
    "train_encodings = tokenizer(train_texts, padding=\"max_length\", truncation=True)\n",
    "test_encodings = tokenizer(test_texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Define model inputs\n",
    "input_ids = Input(shape=(train_encodings[\"input_ids\"].shape[1],), dtype=tf.int32, name=\"input_ids\")\n",
    "\n",
    "# Pass input through pre-trained XLNet model (freeze base layers for initial training)\n",
    "outputs = TFXLNetModel.from_pretrained(model_name)(input_ids, output_hidden_states=True)\n",
    "last_hidden_state = outputs[0]  # Last hidden state from pre-trained model\n",
    "\n",
    "# Freeze the base layers of the pre-trained model (optional, adjust based on task complexity)\n",
    "for layer in model.layers[:12]:  # Adjust number of layers to freeze\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new layers for fine-tuning\n",
    "x = Dense(128, activation=\"relu\")(last_hidden_state)  # Adjust hidden layer size and activation\n",
    "output = Dense(1, activation=\"sigmoid\")(x)  # Adjust for your classification task (e.g., binary)\n",
    "\n",
    "# Create the final fine-tuned model\n",
    "model = Model(inputs=input_ids, outputs=output)\n",
    "\n",
    "# Compile the model (optimizer, loss function, metrics)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model on your Indonesian data (split into training and validation sets)\n",
    "model.fit(\n",
    "    x=train_encodings[\"input_ids\"],\n",
    "    y=train_labels,\n",
    "    validation_split=0.2,  # Adjust validation split ratio\n",
    "    epochs=3,  # Adjust number of epochs based on dataset and task complexity\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_encodings[\"input_ids\"], test_labels)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

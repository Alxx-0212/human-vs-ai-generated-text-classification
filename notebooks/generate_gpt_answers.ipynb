{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import duckdb\n",
    "from preprocess_raw_html import preprocess_raw_html # created function\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Philosophy Questions Stored in DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"philosophy_questions_incremental\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"philosophy_questions\",\n",
    ")\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "print('Loaded tables: ')\n",
    "display(conn.sql(\"show tables\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = conn.sql(\"SELECT * FROM philosophy_questions__items\").df()\n",
    "display(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answered Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "answered_questions = questions[questions[\"is_answered\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answered_questions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "answered_questions_body = answered_questions[[\"body\",\"link\"]].iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answered_questions_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_raw_html(answered_questions_body[\"body\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Answers with OpenAI gpt-35-turbo (model version 0125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint = os.getenv(\"OPENAI_API_ENDPOINT\")\n",
    "    )\n",
    "    \n",
    "deployment_name='gpt-35-turbo' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_answers(client,answered_questions_body,start=0,finish=500,filename=\"gpt35_0125_philosophy_answers.jsonl\"):\n",
    "    idx = start\n",
    "    while True:\n",
    "        if idx==finish:\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Working on question {idx} ...\")\n",
    "            prompt = preprocess_raw_html(answered_questions_body[\"body\"].iloc[idx])\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                            model=\"gpt-35-turbo\", \n",
    "                            messages=[\n",
    "                                {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "                                {\"role\": \"user\", \"content\": prompt}\n",
    "                            ])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Retrying in 60 seconds...\")\n",
    "                with open('openai-gpt35-0125-log.txt', 'a') as f:\n",
    "                    f.write(str(idx)+'-'+\"fail\"+'\\n')  \n",
    "                idx+=1\n",
    "                time.sleep(60)   \n",
    "                continue\n",
    "        \n",
    "        with jsonlines.open(filename, 'a') as writer:\n",
    "            writer.write(response.json())\n",
    "            print(f\"JSON data written to {filename}\")      \n",
    "\n",
    "        with open('openai-gpt35-0125-log.txt', 'a') as f:\n",
    "            f.write(str(idx)+'-'+\"success\"+'\\n')  \n",
    "\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openai-gpt35-0125-log.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        checkpoint = int(line.strip().split(\"-\")[0])\n",
    "\n",
    "get_gpt_answers(client,answered_questions_body,start=checkpoint+1,finish=2000)\n",
    "\n",
    "# code to get the response\n",
    "# print(response.model_dump_json(indent=2))\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('openai-gpt35-0125-log.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         checkpoint = int(line.strip().split(\"-\")[0])\n",
    "\n",
    "# for i in range(checkpoint+1,2000):\n",
    "#     prompt = preprocess_raw_html(answered_questions_body[\"body\"].iloc[i])\n",
    "#     # main dictionary for storing objects for batch processing\n",
    "#     d = {}\n",
    "#     d[\"custom_id\"] = f\"task-{i}\"\n",
    "#     d[\"method\"] = \"POST\"\n",
    "#     d[\"url\"] = \"/chat/completions\"\n",
    "#     # dictionary for storing the body objects\n",
    "#     body = {}\n",
    "#     body[\"model\"] = \"gpt-35-turbo\"\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "#     body[\"messages\"] = messages\n",
    "\n",
    "#     d[\"body\"] = body\n",
    "\n",
    "#     with jsonlines.open(\"openai-gpt35-0125-batch-job.jsonl\", 'a') as writer:\n",
    "#         writer.write(json.dumps(d)+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dlt.resource(table_name=\"gpt35_0125_philosophy_answers\", write_disposition=\"append\")\n",
    "# def get_gpt_answers(client,answered_questions_body,start=0,finish=500):\n",
    "\n",
    "#     checkpoint = dlt.current.resource_state().setdefault(\"checkpoint\", start)\n",
    "#     while True:\n",
    "#         idx = int(dlt.current.resource_state()[\"checkpoint\"])\n",
    "#         if idx==finish:\n",
    "#             break\n",
    "#         else:\n",
    "#             print(f\"Working on question {idx} ...\")\n",
    "#             prompt = preprocess_raw_html(answered_questions_body[\"body\"].iloc[idx])\n",
    "#             try:\n",
    "#                 response = client.chat.completions.create(\n",
    "#                             model=\"gpt-35-turbo\", \n",
    "#                             messages=[\n",
    "#                                 {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "#                                 {\"role\": \"user\", \"content\": prompt}\n",
    "#                             ])\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 print(\"Retrying in 60 seconds...\")\n",
    "#                 time.sleep(60)    \n",
    "#                 continue\n",
    "            \n",
    "#         yield response.json()\n",
    "#         dlt.current.resource_state()[\"checkpoint\"]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = dlt.pipeline(\n",
    "#     pipeline_name=\"gpt35_0125_philosophy_answers_incremental\",\n",
    "#     destination=\"duckdb\",\n",
    "#     dataset_name=\"gpt35_0125_philosophy_answers\",\n",
    "# )\n",
    "\n",
    "# load_info = pipeline.run(get_gpt_answers(client,answered_questions_body))\n",
    "# row_counts = pipeline.last_trace.last_normalize_info\n",
    "\n",
    "# print(row_counts)\n",
    "# print(\"------\")\n",
    "# print(load_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

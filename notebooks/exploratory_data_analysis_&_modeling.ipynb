{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3,venn2,venn2_circles\n",
    "import seaborn as sns\n",
    "import string\n",
    "import skimpy\n",
    "from ydata_profiling import ProfileReport\n",
    "import re\n",
    "import nltk\n",
    "import missingno as msno\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import trange\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/alexxgo21/workspace/Thesis/scripts\")\n",
    "\n",
    "from preprocess_raw_html import preprocess_raw_html\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../dataset/philosophy-qna-with-gpt35answer_v1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(x) for x in df[\"answers\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df,explorative=True)\n",
    "# profile.to_file(\"profile.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df,labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the related columns for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"view_count\",\"answer_count\",\"is_accepted\",\"answer_score\",\"answer_creation_date\",\"answers\",\"question_score\",\"question_creation_date\",\"link\",\"question\",\"title\",\"gpt35_0125_ans\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_raw(arr):\n",
    "    return [len(ans.split()) for ans in arr]\n",
    "\n",
    "# function to clean raw text\n",
    "def preprocess_text_with_stopwords_raw(arr):\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = arr[i].lower()\n",
    "        arr[i] = re.sub(\"[^0-9a-zA-Z]+\", \" \", arr[i])\n",
    "        # text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    return arr\n",
    "\n",
    "# ds[\"length\"] = ds[\"answer\"].str.len()\n",
    "df[\"word_count\"] = df[\"answers\"].apply(lambda x: [preprocess_raw_html(ans) for ans in x])\n",
    "df[\"word_count\"] = df[\"word_count\"].apply(preprocess_text_with_stopwords_raw)\n",
    "df[\"word_count\"] = df[\"word_count\"].apply(word_count_raw)\n",
    "# ds[\"mean_sentence_len\"] = ds[\"answer\"].map(lambda ans: np.mean([len(s) for s in tokenize.sent_tokenize(ans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"question\"] = df[\"question\"].apply(preprocess_raw_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"question\"] = df[\"question\"].apply(preprocess_text_with_stopwords)\n",
    "df[\"question_word_count\"] = df[\"question\"].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"question_word_count\",ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gpt_ans_unpack\"] = df[\"gpt35_0125_ans\"].apply(lambda x: re.sub(r\"\\n\", \" \", x[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"question\"].str.contains(\"water\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_date = df[\"question_creation_date\"]\n",
    "questions_date = questions_date.apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "year = Counter(list(questions_date.dt.year))\n",
    "\n",
    "ax = sns.barplot(x=list(year.keys()),y=list(year.values()),errwidth=0,color=\"#3373cc\")\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "plt.xlabel(\"Tahun\")\n",
    "plt.ylabel(\"Jumlah Pertanyaan\")\n",
    "plt.title(\"Jumlah Pertanyaan Berdasarkan Tahun\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_count_per_question = df[\"answer_count\"]\n",
    "answer_count_per_question = Counter(list(answer_count_per_question))\n",
    "\n",
    "ax = sns.barplot(x=list(answer_count_per_question.keys()),y=list(answer_count_per_question.values()),errwidth=0,color=\"#3373cc\")\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "plt.xlabel(\"Number of Answer\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Number of Answer Per Question\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_score = df[\"question_score\"]\n",
    "question_score = Counter(list(question_score))\n",
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(x=list(question_score.keys()),y=list(question_score.values()),errwidth=0,color=\"#3373cc\")\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "\n",
    "plt.xlabel(\"Question Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Score Per Question\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_number_of_answers(arr,max_ans=2):\n",
    "    if len(arr) > max_ans:\n",
    "        return random.choices(arr,k=max_ans)\n",
    "    return arr\n",
    "    \n",
    "df[\"answers\"] = df[\"answers\"].apply(lambda x: limit_number_of_answers(x))\n",
    "ans = df[\"answers\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = [preprocess_raw_html(answer) for sublist in ans for answer in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df[\"question\"].apply(preprocess_raw_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_ans = df[\"gpt35_0125_ans\"].to_list() \n",
    "gpt_ans = [re.sub(r\"\\n\", \" \", text[\"choices\"][0][\"message\"][\"content\"]) for text in gpt_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "ax = sns.barplot(y=[len(ans),len(gpt_ans)],x=[\"Manusia\",\"GPT-3.5\"],errwidth=0)\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "plt.ylabel(\"Jumlah Jawaban\")\n",
    "plt.title(\"Jumlah Data Jawaban Manusia dan GPT-3.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.concat([pd.DataFrame({\"text\":ans,\"label\":[\"Respon manusia\" for i in range(len(ans))]}),pd.DataFrame({\"text\":gpt_ans,\"label\":[\"Respon model GPT-3.5\" for i in range(len(gpt_ans))]}),pd.DataFrame({\"text\":questions,\"label\":[\"Pertanyaan\" for i in range(len(gpt_ans))]})],ignore_index=True)\n",
    "ds = ds.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length and Word Count Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(ans):\n",
    "    return len(ans.split())\n",
    "\n",
    "# function to clean raw text\n",
    "def preprocess_text_with_stopwords(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^0-9a-zA-Z]+\", \" \", text)\n",
    "    # text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ds[\"length\"] = ds[\"answer\"].str.len()\n",
    "ds[\"word_count\"] = ds[\"text\"].apply(preprocess_text_with_stopwords)\n",
    "ds[\"word_count\"] = ds[\"word_count\"].apply(word_count)\n",
    "# ds[\"mean_sentence_len\"] = ds[\"answer\"].map(lambda ans: np.mean([len(s) for s in tokenize.sent_tokenize(ans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[ds[\"label\"] == \"Respon manusia\"][\"word_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[ds[\"label\"] == \"Respon model GPT-3.5\"][\"word_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[ds[\"label\"] == \"Pertanyaan\"][\"word_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sort_values(by=\"word_count\",ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ds = ds\n",
    "plot_ds = plot_ds.rename(columns={\"word_count\":\"jumlah kata\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Get the current color palette\n",
    "current_palette = sns.color_palette()\n",
    "\n",
    "# Convert the first color to a hex code\n",
    "color = current_palette[1]\n",
    "hex_code = mcolors.to_hex(color)\n",
    "\n",
    "print(hex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(plot_ds[(plot_ds[\"label\"]==\"Respon model GPT-3.5\")|(plot_ds[\"label\"]==\"Respon manusia\")], x='jumlah kata', color='label', barmode='overlay', histnorm='probability density',color_discrete_sequence=[\"#1f77b4\", \"#ff7f0e\"])\n",
    "fig.update_layout(\n",
    "    width=800,  # Set the width of the plot\n",
    "    height=600  # Set the height of the plot\n",
    ")\n",
    "fig.show()\n",
    "# # Add vertical lines for mean, median, and mode\n",
    "# for class_name, stats in grouped_df.iterrows():\n",
    "#     fig.add_vline(x=stats['mean'], line_width=2, line_dash=\"dash\", line_color=\"red\", annotation_text=f\"Mean: {stats['mean']:.2f}\", annotation_position=\"top right\")\n",
    "#     fig.add_vline(x=stats['median'], line_width=2, line_dash=\"dot\", line_color=\"green\", annotation_text=f\"Median: {stats['median']:.2f}\", annotation_position=\"top left\")\n",
    "#     fig.add_vline(x=stats['mode'], line_width=2, line_dash=\"solid\", line_color=\"blue\", annotation_text=f\"Mode: {stats['mode']:.2f}\", annotation_position=\"bottom right\")\n",
    "\n",
    "# # Add range annotations as text\n",
    "# for class_name, stats in grouped_df.iterrows():\n",
    "#     fig.add_annotation(x=stats['min'], y=0.05, text=f\"Range: {stats['range']:.2f}\", showarrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(col):\n",
    "    print()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.boxplot(y=ds[col])\n",
    "    plt.ylabel(col, labelpad=12.5)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    sns.kdeplot(ds[ds[\"label\"]==\"human-generated\"][col])\n",
    "    sns.kdeplot(ds[ds[\"label\"]==\"gpt35-generated\"][col])\n",
    "    plt.legend(ds[\"label\"].unique())\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ds.columns[2:]:\n",
    "    visualize(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean raw text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^0-9a-zA-Z]+\", \" \", text)\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text_remove_nums(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^0-9a-zA-Z]+\", \" \", text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"answer_word_list\"] = ds[\"answer\"].apply(lambda x:preprocess_text(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_corpus = []\n",
    "gpt_corpus = []\n",
    "for i in trange(ds.shape[0], ncols=150, nrows=10, colour='green', smoothing=0.8):\n",
    "    if ds[\"label\"][i] == \"human-generated\":\n",
    "        human_corpus += ds[\"answer_word_list\"][i]\n",
    "    else:\n",
    "        gpt_corpus += ds[\"answer_word_list\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostCommon_human = Counter(human_corpus).most_common(10)\n",
    "mostCommon_gpt = Counter(gpt_corpus).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "freq = []\n",
    "for word, count in mostCommon_human:\n",
    "    words.append(word)\n",
    "    freq.append(count)\n",
    "\n",
    "sns.barplot(x=freq, y=words)\n",
    "plt.title(\"Top 10 Most Frequently Occuring Words in Human-Generated Answers\")\n",
    "plt.show()\n",
    "\n",
    "words = []\n",
    "freq = []\n",
    "for word, count in mostCommon_gpt:\n",
    "    words.append(word)\n",
    "    freq.append(count)\n",
    "\n",
    "sns.barplot(x=freq, y=words)\n",
    "plt.title(\"Top 10 Most Frequently Occuring Words in GPT-Generated Answers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_set = set(human_corpus)\n",
    "gpt_set = set(gpt_corpus)\n",
    "\n",
    "venn2([gpt_set,human_set],('gpt','human'))\n",
    "venn2_circles(subsets=[gpt_set,human_set],linewidth=1,color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(human_corpus+gpt_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "bigrams = cv.fit_transform(ds[ds[\"label\"]==\"human-generated\"][\"answer\"].apply(preprocess_text))\n",
    "count_values = bigrams.sum(axis=0)\n",
    "count_values = np.array(count_values)[0]\n",
    "ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))\n",
    "ngram_freq.columns = [\"frequency\", \"n_gram\"]\n",
    "sns.barplot(x=ngram_freq[\"frequency\"][:10], y=ngram_freq[\"n_gram\"][:10])\n",
    "plt.title(\"Top 10 Most Frequently Occuring Bigrams Human Generated Answers\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "bigrams = cv.fit_transform(ds[ds[\"label\"]==\"gpt35-generated\"][\"answer\"].apply(preprocess_text))\n",
    "count_values = bigrams.sum(axis=0)\n",
    "count_values = np.array(count_values)[0]\n",
    "ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))\n",
    "ngram_freq.columns = [\"frequency\", \"n_gram\"]\n",
    "sns.barplot(x=ngram_freq[\"frequency\"][:10], y=ngram_freq[\"n_gram\"][:10])\n",
    "plt.title(\"Top 10 Most Frequently Occuring Bigrams GPT-35 Generated Answer\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(3,3))\n",
    "trigrams = cv.fit_transform(ds[ds[\"label\"]==\"human-generated\"][\"answer\"].apply(preprocess_text))\n",
    "count_values = trigrams.sum(axis=0)\n",
    "count_values = np.array(count_values)[0]\n",
    "ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))\n",
    "ngram_freq.columns = [\"frequency\", \"n_gram\"]\n",
    "sns.barplot(x=ngram_freq[\"frequency\"][:10], y=ngram_freq[\"n_gram\"][:10])\n",
    "plt.title(\"Top 10 Most Frequently Occuring Trigrams on Human Generated Answer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(3,3))\n",
    "trigrams = cv.fit_transform(ds[ds[\"label\"]==\"gpt35-generated\"][\"answer\"].apply(preprocess_text))\n",
    "count_values = trigrams.sum(axis=0)\n",
    "count_values = np.array(count_values)[0]\n",
    "ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))\n",
    "ngram_freq.columns = [\"frequency\", \"n_gram\"]\n",
    "sns.barplot(x=ngram_freq[\"frequency\"][:10], y=ngram_freq[\"n_gram\"][:10])\n",
    "plt.title(\"Top 10 Most Frequently Occuring Trigrams on GPT-35 Generated Answer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds[\"answer\"].apply(preprocess_text).values\n",
    "Y = ds[\"label\"].values\n",
    "\n",
    "# train test split using StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(X,Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "# feature extraction with TF-IDF\n",
    "vectorizer=TfidfVectorizer(strip_accents = 'ascii', stop_words='english',max_features=5000,ngram_range=(1,5))\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_test_tf = vectorizer.transform(X_test)\n",
    "\n",
    "# encode the label\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(Y_train)\n",
    "\n",
    "Y_train_encoded = label_encoder.transform(Y_train)\n",
    "Y_test_encoded = label_encoder.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True,C=10,gamma=0.1,kernel=\"rbf\")\n",
    "\n",
    "# Hyperparameter grid\n",
    "# param_grid = {\"C\": [0.1, 1, 10], \"kernel\": [\"rbf\",\"sigmoid\",\"polynomial\"], \"gamma\":[0.1,1,10]}\n",
    "# Best Parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
    "# Accuracy: 0.9548872180451128\n",
    "\n",
    "# Grid search\n",
    "# grid_search = GridSearchCV(svc, param_grid, cv=5, verbose=2)\n",
    "\n",
    "# Fit and evaluate\n",
    "model = svc.fit(X_train_tf, Y_train_encoded)\n",
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "Y_pred = model.predict(X_test_tf)\n",
    "\n",
    "accuracy = accuracy_score(Y_test_encoded, Y_pred)\n",
    "\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(classification_report(Y_test_encoded,Y_pred))\n",
    "print(confusion_matrix(Y_test_encoded,Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = make_pipeline(vectorizer, model)\n",
    "\n",
    "ls_X_test= list(X_test)\n",
    "\n",
    "class_names = {0: \"gpt35-generated\", 1:\"human-generated\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# create the LIME explainer\n",
    "# add the class names for interpretability\n",
    "LIME_explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# choose a random single prediction\n",
    "idx = 100\n",
    "# explain the chosen prediction\n",
    "LIME_exp = LIME_explainer.explain_instance(ls_X_test[idx], c.predict_proba)\n",
    "# print results\n",
    "print('Philosophy answer:', ls_X_test[idx])\n",
    "print('Probability human-generated =', c.predict_proba([ls_X_test[idx]]).round(3)[0,1])\n",
    "print('True class: %s' % class_names.get(list(Y_test_encoded)[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the explainability results with highlighted text\n",
    "LIME_exp.save_to_file('SVC.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "Y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(classification_report(Y_test,Y_pred))\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "Y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(classification_report(Y_test,Y_pred))\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import keras_tuner\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "    print(\"GPU is available.\")\n",
    "except RuntimeError:\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = np.array(ds[\"answer\"].apply(preprocess_text).values), np.array(ds[\"label\"].values)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(X,Y):\n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    Y_train, Y_test = Y[train_index], Y[test_index] \n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for val_index, test_index in sss.split(X_test,Y_test):\n",
    "    X_val, X_test = X_test[val_index], X_test[test_index]\n",
    "    Y_val, Y_test = Y_test[val_index], Y_test[test_index] \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(Y_train)\n",
    "\n",
    "Y_train = label_encoder.transform(Y_train) \n",
    "Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
    "\n",
    "Y_val = label_encoder.transform(Y_val) \n",
    "Y_val = Y_val.reshape(Y_val.shape[0],1)\n",
    "\n",
    "Y_test = label_encoder.transform(Y_test) \n",
    "Y_test = Y_test.reshape(Y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 500\n",
    "trunc_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "validation_sequences = tokenizer.texts_to_sequences(X_val)  \n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "training_set = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "validation_set = pad_sequences(validation_sequences, maxlen=max_length)\n",
    "testing_set = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with embedding layer\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")  \n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "history = model.fit(training_set, Y_train, epochs=100, validation_data=(validation_set, Y_val), batch_size=64, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot(epochs, acc, 'r',label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', label=\"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the testing set\n",
    "Y_pred = model.predict(testing_set)\n",
    "Y_pred = (Y_pred > 0.5).astype(int)  # Assuming a binary classification with a threshold of 0.5\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(Y_test, Y_pred, target_names=label_encoder.classes_)\n",
    "print(report)\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length))\n",
    "    layer_type = hp.Choice(\"layer_type\", [\"conv1d\", \"lstm\", \"bilstm\"])\n",
    "    if layer_type==\"conv1d\":\n",
    "        model.add(tf.keras.layers.Conv1D(filters=hp.Int(\"filters\",min_value=32,max_value=128,step=32),kernel_size=5,activation=\"relu\"))\n",
    "    elif layer_type==\"lstm\":\n",
    "        num_layers = hp.Int('num_layers', min_value=1, max_value=2, step=1)\n",
    "        for i in range(int(num_layers)):\n",
    "            if i<(int(num_layers)-1):\n",
    "                model.add(tf.keras.layers.LSTM(hp.Int(\"units\",min_value=32,max_value=64,step=32),return_sequences=True))\n",
    "            else:\n",
    "                model.add(tf.keras.layers.LSTM(hp.Int(\"units\",min_value=32,max_value=64,step=32)))\n",
    "    else:\n",
    "        num_layers = hp.Int('num_layers', min_value=1, max_value=2, step=1)\n",
    "        for i in range(int(num_layers)):\n",
    "            if i<(int(num_layers)-1):\n",
    "                model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp.Int(\"units\",min_value=32,max_value=64,step=32),return_sequences=True)))\n",
    "            else:\n",
    "                model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp.Int(\"units\",min_value=32,max_value=64,step=32))))\n",
    "\n",
    "    if layer_type==\"conv1d\":\n",
    "        layer_type = hp.Choice(\"layer_type\", [\"flatten\", \"global_avg_pooling\", \"max_pooling\"])\n",
    "        if layer_type==\"flatten\":\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "        elif layer_type==\"global_avg_pooling\":\n",
    "            model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "        else:\n",
    "            model.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "\n",
    "    num_layers = hp.Int('num_layers', min_value=1, max_value=2, step=1)\n",
    "    for i in range(num_layers):\n",
    "        model.add(tf.keras.layers.Dense(hp.Int('units', min_value=32, max_value=128, step=32), activation=\"relu\"))\n",
    "        if hp.Boolean('use_dropout'):\n",
    "            model.add(tf.keras.layers.Dropout(hp.Float('dropout_rate', min_value=0.1, max_value=0.2, step=0.1)))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate',[0.001, 0.005, 0.01])), metrics=[\"accuracy\"]) \n",
    "\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.Hyperband(build_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='hyperparameter_tuning',\n",
    "                     project_name='Sequence Model Hyperparameter Tuning')\n",
    "\n",
    "# tuner.search(training_set, Y_train, epochs=100, validation_data=(validation_set, Y_val), batch_size=32, callbacks=[callback])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()  \n",
    "# tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "plot_model(hypermodel,show_shapes=True, show_layer_names=True)\n",
    "# hypermodel.fit(training_set, Y_train, epochs=100, validation_data=(validation_set, Y_val), batch_size=64, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the testing set\n",
    "Y_pred = hypermodel.predict(testing_set)\n",
    "Y_pred = (Y_pred > 0.5).astype(int)  # Assuming a binary classification with a threshold of 0.5\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(Y_test, Y_pred, target_names=label_encoder.classes_)\n",
    "print(report)\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model with XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import XLNetForSequenceClassification, pipeline, TextClassificationPipeline\n",
    "from transformers import AdamW, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"answer\"] = ds[\"answer\"].apply(preprocess_text)\n",
    "ds[\"label\"] = ds[\"label\"].apply(lambda x: 1 if x==\"human-generated\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ds.answer.values\n",
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "label = ds.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet/xlnet-base-cased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = np.array(input_ids)\n",
    "label = np.array(label)\n",
    "attention_masks = np.array(attention_masks)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(input_ids,label):\n",
    "    train_inputs, validation_inputs = input_ids[train_index], input_ids[test_index] \n",
    "    train_labels, validation_labels = label[train_index], label[test_index] \n",
    "    train_masks, validation_masks = attention_masks[train_index], attention_masks[test_index]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for validation_index, test_index in sss.split(validation_inputs,validation_labels):\n",
    "    validation_inputs, test_inputs = validation_inputs[validation_index], validation_inputs[test_index] \n",
    "    validation_labels, test_labels = validation_labels[validation_index], validation_labels[test_index] \n",
    "    validation_masks, test_masks = validation_masks[validation_index], validation_masks[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(list(train_inputs))\n",
    "validation_inputs = torch.tensor(list(validation_inputs))\n",
    "test_inputs = torch.tensor(list(test_inputs))\n",
    "\n",
    "train_labels = torch.tensor(list(train_labels))\n",
    "validation_labels = torch.tensor(list(validation_labels))\n",
    "test_labels = torch.tensor(list(test_labels))\n",
    "\n",
    "train_masks = torch.tensor(list(train_masks))\n",
    "validation_masks = torch.tensor(list(validation_masks))\n",
    "test_masks = torch.tensor(list(test_masks))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XLNEtForSequenceClassification, the pretrained XLNet model with a single linear classification layer on top. \n",
    "\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet/xlnet-base-cased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      logits = output[0]\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "precision = precision_score(flat_true_labels, flat_predictions)\n",
    "recall = recall_score(flat_true_labels, flat_predictions)\n",
    "f1 = f1_score(flat_true_labels, flat_predictions)\n",
    "conf_matrix = confusion_matrix(flat_true_labels, flat_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"xlnet_model_v1 (MAX_LEN=128,batch_size=32,epochs=4).pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Test with Trained XLNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "base_model = torch.load(\"./xlnet_model_v1 (MAX_LEN=128,batch_size=32,epochs=4).pth\") \n",
    "\n",
    "# Put model in evaluation mode\n",
    "base_model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    outputs = base_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "precision = precision_score(flat_true_labels, flat_predictions)\n",
    "recall = recall_score(flat_true_labels, flat_predictions)\n",
    "f1 = f1_score(flat_true_labels, flat_predictions)\n",
    "conf_matrix = confusion_matrix(flat_true_labels, flat_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# classifier = TextClassificationPipeline(model=base_model,tokenizer=tokenizer,return_all_scores=True)\n",
    "# explainer = shap.Explainer(classifier)\n",
    "# shap_values = explainer([sentences[1]])\n",
    "# shap.plots.text(shap_values[:,:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
